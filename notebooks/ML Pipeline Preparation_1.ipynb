{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "\n",
    "###  Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/eddadsi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/eddadsi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/eddadsi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/eddadsi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine \n",
    "import re\n",
    "\n",
    "# sklearn \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download(['punkt', 'wordnet', 'stopwords' ,\n",
    "               'averaged_perceptron_tagger'])\n",
    "\n",
    "# Enable pep8 code style test\n",
    "%load_ext pycodestyle_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///DisasterResponse.db')\n",
    "df = pd.read_sql('Messages', con=engine)\n",
    "X = df.message\n",
    "Y = df.drop(labels=['id', 'message', 'original', 'genre'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # normalize case and remove punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "\n",
    "    # tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # lemmatize and remove stop words\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens\n",
    "              if word not in stop_words]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a starting verbe extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StartingVerbExtractor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def starting_verb(self, text):\n",
    "        sentence_list = nltk.sent_tokenize(text)\n",
    "        if len(sentence_list) != 0:\n",
    "            for sentence in sentence_list:\n",
    "                pos_tags = nltk.pos_tag(tokenize(sentence))\n",
    "                if len(pos_tags) != 0:\n",
    "                    first_word, first_tag = pos_tags[0]\n",
    "                    if first_tag in ['VB', 'VBP', 'VBG'] or first_word == 'RT':\n",
    "                        return True\n",
    "            return False\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_tagged = pd.Series(X).apply(self.starting_verb)\n",
    "        return pd.DataFrame(X_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a length message extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LenMessageExtractor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return len(tokenize(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "\n",
    "        ('text_pipeline', Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                ('tfidf', TfidfTransformer())\n",
    "            ])),\n",
    "\n",
    "        ('starting_verb', StartingVerbExtractor()),\n",
    "\n",
    "        ('length_text', StartingVerbExtractor())\n",
    "\n",
    "    ])),\n",
    "\n",
    "    ('clf', MultiOutputClassifier(\n",
    "        RandomForestClassifier(\n",
    "            n_estimators=10, random_state=42)))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X.values, Y.values)\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Test  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "y_pred = pipeline.predict(X_test[:20])\n",
    "\n",
    "class_names = list(Y.columns)\n",
    "\n",
    "\n",
    "def calcul_metrics(y_test, y_pred):\n",
    "    dic_metrics = {'precision': [], 'recall': [], 'f1-score': []}\n",
    "    for column in range(y_pred.shape[1]):\n",
    "        metrics = classification_report(y_test[:, 0], y_pred[:, 0],\n",
    "                                        output_dict=True)['micro avg']\n",
    "        dic_metrics['precision'].append(metrics['precision'])\n",
    "        dic_metrics['recall'].append(metrics['recall'])\n",
    "        dic_metrics['f1-score'].append(metrics['f1-score'])\n",
    "\n",
    "    # Stock metrics in a Dataframe\n",
    "    df_metrics = pd.DataFrame(dic_metrics, index=class_names)\n",
    "    return df_metrics.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eddadsi/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "precision    0.9\n",
       "recall       0.9\n",
       "f1-score     0.9\n",
       "dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# metrics\n",
    "calcul_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametres\n",
    "parameters = {\n",
    "    'features__text_pipeline__vect__ngram_range': ((1, 1), (1, 2)),\n",
    "    'features__text_pipeline__vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'features__text_pipeline__vect__max_features': (None, 5000, 10000),\n",
    "    'features__text_pipeline__tfidf__use_idf': (True, False),\n",
    "    'clf__estimator__n_estimators': [10, 20, 30, 50, 100],\n",
    "    'clf__estimator__min_samples_split': [2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "# GridSearchCV\n",
    "cv = GridSearchCV(pipeline, param_grid=parameters, cv=3, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.fit(X_train, y_train)\n",
    "y_pred = cv.predict(X_test)\n",
    "calcul_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results metrics\n",
    "calcul_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Parameters: {'clf__estimator__min_samples_split': 2, 'clf__estimator__n_estimators': 50, 'features__text_pipeline__tfidf__use_idf': True, 'features__text_pipeline__vect__max_df': 0.5, 'features__text_pipeline__vect__max_features': None, 'features__text_pipeline__vect__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "# print model best Parameters\n",
    "print(\"\\nBest Parameters:\", cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the classifier\n",
    "filename = 'digits_classifier.joblib.pkl'\n",
    "_ = joblib.dump(cv, filename, compress=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load it again\n",
    "#clf2 = joblib.load(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
